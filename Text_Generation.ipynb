{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c880b6fdeb244a3299583bcb021fff64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecb9bf0d7854a11ba171f93febb6076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566cd90885d44e71b4992700e4fbfa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1236d2fb88504a478467a7bae1ebbff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23192018f1c4ab9ba026b1447b4bb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8947bfee184c41be3f94fb28731f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ddf68c766343958d64c3bf239d137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "transformers.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Natural language processing is a growing domain in machine learning\"\n",
    "\n",
    "synthentic_text = text_generator(input_text, max_length=100, truncation=True, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing is a growing domain in machine learning and semantic processing. It will often be referred to as the \"human-machine-learning\" field, with examples of it in a variety of fields from social psychology and neuroscience to computational neuroscience and computational biology. As of 2015, many of these developments are covered in this section.\n",
      "\n",
      "Computer vision (CD) can be used to explore and interpret human facial features and to detect and classify faces when the brain is performing sensory interactions. Most CD projects\n",
      "\n",
      "\n",
      "________________________\n",
      "Natural language processing is a growing domain in machine learning industry. Some recent advances in AI algorithms have also led to increased usage of such AI systems as artificial colors, animation, and voice-activated speech. Although not all of these innovations are as clear cut as these earlier technology advances take on in AI applications, these improvements have raised general excitement and interest among engineers with AI technologies.\n",
      "\n",
      "There has been a lot of interest in AI advancements in recent years. For example, the advancement of deep learning algorithms\n",
      "\n",
      "\n",
      "________________________\n",
      "Natural language processing is a growing domain in machine learning. AI might not seem difficult at first, but then it comes to mind where it could be most effective at understanding human behavior.\n",
      "\n",
      "And though the current state of our knowledge of speech processing and language processing is pretty good, it must be considered with some consideration on how AI could be used to solve problems.\n",
      "\n",
      "At the onset of technology, when people talk about \"smart things,\" a very strong, concrete, and easy concept. Often\n",
      "\n",
      "\n",
      "________________________\n"
     ]
    }
   ],
   "source": [
    "for text in synthentic_text:\n",
    "    print(text['generated_text'])\n",
    "    print(\"\\n\\n________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9a6c723f174d11a89bfca4fcc44577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f476a9bfccf54765923e408eb6c0d790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/311 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3e161aa5474aa5b3f0e64fb3033fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e26bfe7cf74b0fbd99b35606d89759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/964k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957f51e10c8c4ef3b4bf7b9db9d8f426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/345k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3e20f14b2d487faf6eefc4d96226f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversational_pipeline = pipeline(\"conversational\", \n",
    "                                   model=\"facebook/blenderbot_small-90M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlenderbotSmallConfig {\n",
       "  \"_name_or_path\": \"facebook/blenderbot_small-90M\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BlenderbotSmallForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 2048,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 8,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": true,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 2048,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 8,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"extra_pos_embeddings\": 0,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layernorm_variant\": \"xlm\",\n",
       "  \"length_penalty\": 0.65,\n",
       "  \"max_length\": 128,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"min_length\": 20,\n",
       "  \"model_type\": \"blenderbot-small\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 10,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"scale_embedding\": true,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"unk_token_id\": 3,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 54944\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_pipeline.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the BlenderbotSmallTokenizer class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Exchange: \n",
      "--------------------\n",
      " User Input: Do you have any hobbies?\n",
      " Bot Output: yes, i love going to the beach. what about you? do you have any hobbies?\n",
      "\n",
      "Second Exchange: \n",
      "--------------------\n",
      " User Input: I like to watch movies\n",
      " Bot Output: i love going to the beach. i also like to watch movies. what kind of movies do you like?\n",
      "\n",
      "Third Exchange: \n",
      "--------------------\n",
      " User Input: action movies\n",
      " Bot Output: i love going to the beach. i also like to watch movies. what kind of movies do you like?\n",
      "\n",
      "Accessing All Responses: \n",
      "Conversation id: 9ebf1411-352e-4452-a3a0-afd266982db3\n",
      "user: Do you have any hobbies?\n",
      "assistant: yes, i love going to the beach. what about you? do you have any hobbies?\n",
      "user: I like to watch movies\n",
      "assistant: i love going to the beach. i also like to watch movies. what kind of movies do you like?\n",
      "user: action movies\n",
      "assistant: i love going to the beach as well. i like action movies as well, but i don't get to see them often.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sample inputs\n",
    "first_input=\"Do you have any hobbies?\"\n",
    "second_input = \"I like to watch movies\"\n",
    "third_input = \"action movies\"\n",
    "\n",
    "#Create a context\n",
    "bot_conversation = Conversation(first_input)\n",
    "\n",
    "print(\"\\nFirst Exchange: \\n--------------------\")\n",
    "\n",
    "conversational_pipeline(bot_conversation)\n",
    "print(\" User Input:\", bot_conversation.past_user_inputs[0])\n",
    "print(\" Bot Output:\", bot_conversation.generated_responses[0])\n",
    "\n",
    "print(\"\\nSecond Exchange: \\n--------------------\")\n",
    "bot_conversation.add_user_input(second_input)\n",
    "conversational_pipeline(bot_conversation)\n",
    "\n",
    "print(\" User Input:\", bot_conversation.past_user_inputs[1])\n",
    "print(\" Bot Output:\", bot_conversation.generated_responses[1])\n",
    "\n",
    "print(\"\\nThird Exchange: \\n--------------------\")\n",
    "bot_conversation.add_user_input(third_input)\n",
    "conversational_pipeline(bot_conversation)\n",
    "\n",
    "print(\" User Input:\", bot_conversation.past_user_inputs[2])\n",
    "print(\" Bot Output:\", bot_conversation.generated_responses[1])\n",
    "\n",
    "print(\"\\nAccessing All Responses: \")\n",
    "print(bot_conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd565c7e75546c78f10b45739ad2744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56737ddca2f4eb99cc1b6690a09ccb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_english = \"I am learning natural language processing using hugging face models in the year two thousand twenty four\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs_german = auto_tokenizer(\n",
    "    \"translate English to German: \" + source_english,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs_german = model.generate(\n",
    "    inputs_german[\"input_ids\"], \n",
    "    max_length=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"German Translation: {auto_tokenizer.decode(outputs_german[0], \n",
    "                       skip_special_tokens=True)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_french = auto_tokenizer(\n",
    "    \"translate English to French: \" + source_english, \n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs_french = model.generate(\n",
    "    inputs_french[\"input_ids\"], \n",
    "    max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"French Translation: {auto_tokenizer.decode(outputs_french[0], \n",
    "                       skip_special_tokens=True)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
